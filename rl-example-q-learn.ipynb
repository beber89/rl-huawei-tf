{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, num_states, num_actions, batch_size):\n",
    "        self._num_states = num_states\n",
    "        self._num_actions = num_actions\n",
    "        self._batch_size = batch_size\n",
    "        # define the placeholders\n",
    "        self._states = None\n",
    "        self._actions = None\n",
    "        # the output operations\n",
    "        self._logits = None\n",
    "        self._optimizer = None\n",
    "        self._var_init = None\n",
    "        # now setup the model\n",
    "        self._define_model()\n",
    "\n",
    "    def _define_model(self):\n",
    "        self._states = tf.placeholder(shape=[None, self._num_states], dtype=tf.float32)\n",
    "        self._q_s_a = tf.placeholder(shape=[None, self._num_actions], dtype=tf.float32)\n",
    "        # create a couple of fully connected hidden layers\n",
    "        fc1 = tf.layers.dense(self._states, 50, activation=tf.nn.relu)\n",
    "        fc2 = tf.layers.dense(fc1, 50, activation=tf.nn.relu)\n",
    "        self._logits = tf.layers.dense(fc2, self._num_actions)\n",
    "        loss = tf.losses.mean_squared_error(self._q_s_a, self._logits)\n",
    "        self._optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "        self._var_init = tf.global_variables_initializer()\n",
    "    \n",
    "    def predict_one(self, state, sess):\n",
    "        return sess.run(self._logits, feed_dict={self._states:\n",
    "                                                 state.reshape(1, self.num_states)})\n",
    "    def predict_batch(self, states, sess):\n",
    "        return sess.run(self._logits, feed_dict={self._states: states})\n",
    "    \n",
    "    def train_batch(self, sess, x_batch, y_batch):\n",
    "        sess.run(self._optimizer, feed_dict={self._states: x_batch, self._q_s_a: y_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_memory):\n",
    "        self._max_memory = max_memory\n",
    "        self._samples = []\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "        if len(self._samples) > self._max_memory:\n",
    "            self._samples.pop(0)\n",
    "\n",
    "    def sample(self, no_samples):\n",
    "        if no_samples > len(self._samples):\n",
    "            return random.sample(self._samples, len(self._samples))\n",
    "        else:\n",
    "            return random.sample(self._samples, no_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameRunner:\n",
    "    def __init__(self, sess, model, env, memory, max_eps, min_eps,\n",
    "                 decay, render=True):\n",
    "        self._sess = sess\n",
    "        self._env = env\n",
    "        self._model = model\n",
    "        self._memory = memory\n",
    "        self._render = render\n",
    "        self._max_eps = max_eps\n",
    "        self._min_eps = min_eps\n",
    "        self._decay = decay\n",
    "        self._eps = self._max_eps\n",
    "        self._steps = 0\n",
    "        self._reward_store = []\n",
    "        self._max_x_store = []\n",
    "    \n",
    "    def _choose_action(self, state):\n",
    "        if random.random() < self._eps:\n",
    "            return random.randint(0, self._model.num_actions - 1)\n",
    "        else:\n",
    "            return np.argmax(self._model.predict_one(state, self._sess))\n",
    "        \n",
    "    def _replay(self):\n",
    "        batch = self._memory.sample(self._model.batch_size)\n",
    "        states = np.array([val[0] for val in batch])\n",
    "        next_states = np.array([(np.zeros(self._model.num_states)\n",
    "                                 if val[3] is None else val[3]) for val in batch])\n",
    "        # predict Q(s,a) given the batch of states\n",
    "        q_s_a = self._model.predict_batch(states, self._sess)\n",
    "        # predict Q(s',a') - so that we can do gamma * max(Q(s'a')) below\n",
    "        q_s_a_d = self._model.predict_batch(next_states, self._sess)\n",
    "        # setup training arrays\n",
    "        x = np.zeros((len(batch), self._model.num_states))\n",
    "        y = np.zeros((len(batch), self._model.num_actions))\n",
    "        for i, b in enumerate(batch):\n",
    "            state, action, reward, next_state = b[0], b[1], b[2], b[3]\n",
    "            # get the current q values for all actions in state\n",
    "            current_q = q_s_a[i]\n",
    "            # update the q value for action\n",
    "            if next_state is None:\n",
    "                # in this case, the game completed after action, so there is no max Q(s',a')\n",
    "                # prediction possible\n",
    "                current_q[action] = reward\n",
    "            else:\n",
    "                current_q[action] = reward + GAMMA * np.amax(q_s_a_d[i])\n",
    "            x[i] = state\n",
    "            y[i] = current_q\n",
    "        self._model.train_batch(self._sess, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_name = 'MountainCar-v0'\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    num_states = env.env.observation_space.shape[0]\n",
    "    num_actions = env.env.action_space.n\n",
    "\n",
    "    model = Model(num_states, num_actions, BATCH_SIZE)\n",
    "    mem = Memory(50000)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(model.var_init)\n",
    "        gr = GameRunner(sess, model, env, mem, MAX_EPSILON, MIN_EPSILON,\n",
    "                        LAMBDA)\n",
    "        num_episodes = 300\n",
    "        cnt = 0\n",
    "        while cnt < num_episodes:\n",
    "            if cnt % 10 == 0:\n",
    "                print('Episode {} of {}'.format(cnt+1, num_episodes))\n",
    "            gr.run()\n",
    "            cnt += 1\n",
    "        plt.plot(gr.reward_store)\n",
    "        plt.show()\n",
    "        plt.close(\"all\")\n",
    "        plt.plot(gr.max_x_store)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
